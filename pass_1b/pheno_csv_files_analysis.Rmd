---
title: 'PASS1B DMAQC data: analysis by BIC'
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document:
    number_sections: yes
---

```{r,message=FALSE,warning=FALSE,results="hide"}
# Set the working directory to the folder with the data
source("~/Desktop/repos/motrpac-bic-norm-qc/tools/gcp_functions.R")

# Load data from the from the buckets
local_path = "~/Desktop/MoTrPAC/data/pass_1b/dmaqc_pheno/"
system(paste("mkdir",local_path))
bucket = "gs://motrpac-portal-transfer-dmaqc/DMAQC_TRANSFER_PASS_1B.6M_1.00/"
dmaqc_data_dir = paste(local_path,"3-Data_Sets/",sep="")
# dictionary path
dmaqc_dict_dir = paste(local_path,"1-Data_Dictionary/",sep="")
download_bucket_files_to_local_dir(bucket = paste(bucket,"3-Data_Sets/",sep=""),
                                   local_path = dmaqc_data_dir)
download_bucket_files_to_local_dir(bucket = paste(bucket,"1-Data_Dictionary/",sep=""),
                                   local_path = dmaqc_dict_dir)

all_csvs = list.files(dmaqc_data_dir,full.names = T) # get all files in dir
all_csvs = all_csvs[grepl(".csv$",all_csvs)] # make sure we take csv only
# read all files
csv_data = list()
for(fname in all_csvs){
  fname_wo_path = strsplit(fname,split='/')[[1]]
  fname_wo_path = fname_wo_path[length(fname_wo_path)]
  csv_data[[fname_wo_path]] = read.csv(fname,stringsAsFactors = F)
}# sapply(csv_data,dim) # check the dimensions of the different datasets

all_dict_csvs = list.files(dmaqc_dict_dir,full.names = T) # get all files in dir
all_dict_csvs = all_dict_csvs[grepl(".csv$",all_dict_csvs)] # make sure we take csv only
# read all files
dict_data = list()
for(fname in all_dict_csvs){
  fname_wo_path = strsplit(fname,split='/')[[1]]
  fname_wo_path = fname_wo_path[length(fname_wo_path)]
  dict_data[[fname_wo_path]] = read.csv(fname,stringsAsFactors = F)
} 

# Shorten the csv names
names(csv_data) = 
  sapply(names(csv_data),function(x)strsplit(x,split="PASS_Animal.")[[1]][2])
names(dict_data) = 
  sapply(names(dict_data),function(x)strsplit(x,split="PASS_Animal.")[[1]][2])
names(csv_data) = gsub(".csv","",names(csv_data))
names(dict_data) = gsub(".csv","",names(dict_data))

#Look at the sites
site_names = c("910"="Joslin","930"="Florida","940"="Iowa")
print(table(site_names[as.character(csv_data$Training$siteID)]))

# Reorder the datasets in the following order: 
# pid-based, labelid-based, bid-based, vial-based, BIC
neword = c(
  "Key","Registration","Familiarization",
  "NMR.Testing", "Training", "Terminal.Weight", "VO2.Max.Test",
  "Specimen.Collection","Specimen.Processing","Calculated.Variables",
  "BICLabelData"
)
csv_data = csv_data[neword]
dict_data = dict_data[neword]
sapply(csv_data,function(x)colnames(x)[1:2])
sapply(csv_data, nrow)
sapply(csv_data,function(x)"pid" %in% names(x))
sapply(csv_data,function(x)"labelid" %in% names(x)) # use labelid in data
sapply(csv_data,function(x)"labelID" %in% names(x)) # use labelID in the BIC's sheet?
sapply(csv_data,function(x)"bid" %in% names(x))
sapply(csv_data,function(x)"vialLabel" %in% names(x))
```

NOTE: in the code above we ignored the site-level tables. These include additional information such as temperature and humidity.

Check field overlaps between the dictionaries. 
Let S be the set of datasets. For a dataset $i$ define a set of fields $R_i$ as the set of fields that already appeared in datasets $1...i-1$. Use the fieldGUID to check for field overlaps. $R_i$ will be used later when the tables are merged. 

```{r}
R = list()
for(n1 in names(dict_data)){
  R_n1 = c()
  for(n2 in names(dict_data)){
    if(n1==n2){break}
    inter = intersect(dict_data[[n1]][,"Field.Name"],
                      dict_data[[n2]][,"Field.Name"])
    if(length(inter)==0){next}
    # print(paste(n1,n2,sep=" ### "))
    # print(inter)
    if(any(grepl("fieldGUID",names(dict_data[[n1]]))) &&
       any(grepl("fieldGUID",names(dict_data[[n2]])))){
          inter = intersect(dict_data[[n1]][,13],
                      dict_data[[n2]][,13])
          if(length(inter)==0){next}
          # print(paste(n1,n2,sep=" ### "))
          # print(inter)
          R_n1 = union(R_n1,
                 unique(dict_data[[n1]][dict_data[[n1]][,13] %in% inter,"Field.Name"]))
       }
  }
  R[[n1]] = R_n1
}
R
```

We go over the data dictionary and mark the fields that we need to keep. To this end we decided to keep fields with codes 1 and 2 in the code for release, column 1 in the dictionary. We decided to exclude fields associated with the CRF version used.

```{r}
new_dicts = list()
for(dictname in names(dict_data)){
  dict = dict_data[[dictname]]
  to_keep = dict[[1]] < 3
  fnames = dict[["Field.Name"]]
  to_keep = to_keep & !grepl("version",fnames)
  dict_columns_to_keep = 
    !grepl("^External.Data.Release",names(dict),perl=T) &
    !grepl("^versionNbr",names(dict),perl=T) &
    !grepl("^Data.Set.Variable.Sequence",names(dict),perl=T) &
    !grepl("^fieldGUID",names(dict),perl=T)
  new_dicts[[dictname]] = unique(dict[to_keep,dict_columns_to_keep])
  if(any(table(new_dicts[[dictname]][,"Field.Name"])>1)){
    print(paste("In",dictname,"field names are not unique after filter"))
    print(new_dicts[[dictname]][,"Field.Name"])
  }
  if(!all(new_dicts[[dictname]][,"Field.Name"] %in% colnames(csv_data[[dictname]]))){
    print(paste("Error, not all remaining fields appear in the dataset:",dictname))
  }
}
dict_data = new_dicts
for(dataset_name in names(csv_data)){
  d = csv_data[[dataset_name]]
  d = d[,dict_data[[dataset_name]][,"Field.Name"]]
  csv_data[[dataset_name]] = d
}
sapply(csv_data, nrow)
sapply(csv_data,function(x)"pid" %in% names(x))
sapply(csv_data,function(x)"labelid" %in% names(x)) # use labelid in data
sapply(csv_data,function(x)"labelID" %in% names(x)) # use labelID in the BIC's sheet?
sapply(csv_data,function(x)"bid" %in% names(x))
sapply(csv_data,function(x)"vialLabel" %in% names(x))

# For some reason some dictionaries start with FormName and some call it Data.Set.Name
sapply(dict_data,names)
for(n in names(dict_data)){
  names(dict_data[[n]])[1] = "FormName"
}
```

# Format the metadata table according to vial ids

Use DMAQC's mapping of label ids to vial ids and use it to generate a single metadata table that we can share with other sites.

```{r,out.height='50%',out.width='50%',eval=F}
# Define the columns that can be ids in the final table
ID_COLS = c("bid","pid","labelID","vialLabel","labelid","viallabel")
DICT_COLS = names(dict_data$Registration)
# helper function for adding fields to the dictionary
get_fields_to_dict<-function(newfields,d2,cols){
  m = d2[d2[,"Field.Name"] %in% newfields,]
  for(col in setdiff(cols,names(d2))){
    m[[col]] = NA
  }
  rownames(m) = m[,"Field.Name"]
  m = m[newfields,]
  rownames(m) = NULL
  return(m)
}

merged_dmaqc_data = c()
merged_dmaqc_dict = c()
# Start with merging by pid
for(currname in names(dict_data)[1:9]){
  curr_data = csv_data[[currname]]
  if(currname %in% names(R)){
    curr_data_subset = curr_data[,
         setdiff(names(curr_data),R[[currname]])]
  }
  else{
    curr_data_subset = curr_data
  }
  
  original_names = names(curr_data_subset)
  is_pid = colnames(curr_data_subset) == "pid"
  colnames(curr_data_subset)[!is_pid] = paste(currname,colnames(curr_data_subset)[!is_pid],sep=".")
  if(sum(is_pid)==0){
    original_names = original_names[original_names!="pid"]
    curr_data_subset[["pid"]] = curr_data[["pid"]]
  }

  if(length(merged_dmaqc_data)==0){
    merged_dmaqc_data = curr_data_subset
  }
  else{
    merged_dmaqc_data = merge(merged_dmaqc_data,curr_data_subset,by="pid")
    print(dim(merged_dmaqc_data))
    print(dim(merged_dmaqc_dict))
  }
  
  merged_dmaqc_dict = rbind(merged_dmaqc_dict,
        get_fields_to_dict(original_names,dict_data[[currname]],DICT_COLS)
  )
}

# Add the calculated variables using the label id
currname = "Calculated.Variables"
curr_data = csv_data[[currname]]
curr_data_subset = curr_data[,
      setdiff(names(curr_data),ID_COLS)]
by_col = "labelid"
colnames(curr_data_subset) = paste(currname,colnames(curr_data_subset),sep=".")
for(id_col in intersect(ID_COLS,names(curr_data))){
      curr_data_subset[[id_col]] = curr_data[[id_col]]
}
# print(paste("merging in table:",currname,", by col:", by_col))
merged_dmaqc_data = merge(merged_dmaqc_data,curr_data_subset,by=by_col)
print("Merged animal and biospecimen data tables, dim is:")
print(dim(merged_dmaqc_data))

# Now map DMAQC's label ids to vialids
mapping_info = csv_data$BICLabelData
colnames(mapping_info) = tolower(colnames(mapping_info))
# Not all samples in the specimen data are necessarily covered in the mapping
# file. The mapping file contains info only about samples that were shipped
# to CAS. As can be seen here:
table(is.element(merged_dmaqc_data$labelid,set=mapping_info$labelid))
# We therefore need to extract the intersection:
shared_labelids = intersect(merged_dmaqc_data$labelid,mapping_info$labelid)
merged_dmaqc_data = merged_dmaqc_data[
  is.element(merged_dmaqc_data$labelid,set = shared_labelids),]
mapping_info = mapping_info[
  is.element(mapping_info$labelid,set = shared_labelids),c("labelid","viallabel")]
print("Merged animal and biospecimen data tables, new dim is:")
print(dim(merged_dmaqc_data))
# We also have a many to one mapping from vial ids to labels, we 
# merge the tables to avoid information loss
merged_dmaqc_data = merge(merged_dmaqc_data,mapping_info,by="labelid")
print("Merged animal and biospecimen data tables, after adding vialids, new dim is:")
print(dim(merged_dmaqc_data))

```

# Save the merged datasets in the cloud

```{r,eval=F}
# Solve some formatting issues and columns that are redundant or wrong
merged_column_dictionary = as.matrix(merged_column_dictionary)
# All NAs are ""
merged_column_dictionary[is.na(merged_column_dictionary)] = ""
# Remove duplications
merged_column_dictionary = unique(merged_column_dictionary)
# Add the vial label
merged_column_dictionary = rbind(
  c("viallabel","The primary analyzed sample id, corresponds to a row in the data table",
    "varchar",NA,NA,NA,NA,""),
  merged_column_dictionary
)
# remove: deathtime_after_acute and and frozetime_after_acute
ind1 = which(grepl("deathtime_after_acute",merged_column_dictionary[,1]))
ind2 = which(grepl("deathtime_after_acute",colnames(merged_dmaqc_data)))
merged_column_dictionary = merged_column_dictionary[-ind1,]
merged_dmaqc_data = merged_dmaqc_data[,-ind2]
ind1 = which(grepl("frozetime_after_acute",merged_column_dictionary[,1]))
ind2 = which(grepl("frozetime_after_acute",colnames(merged_dmaqc_data)))
merged_column_dictionary = merged_column_dictionary[-ind1,]
merged_dmaqc_data = merged_dmaqc_data[,-ind2]

# To see the bucket list
# gsutil ls -p motrpac-portal-dev
currdate = Sys.Date()
txtname = paste("merged_dmaqc_data",currdate,".txt",sep="")
rdataname = paste("merged_dmaqc_data",currdate,".RData",sep="")
dictfname = paste("merged_column_dictionary",currdate,".txt",sep="")
write.table(merged_dmaqc_data,file=txtname,
            quote = F,sep="\t",row.names = F)
save(merged_dmaqc_data,file=rdataname)
write.table(merged_column_dictionary,file=dictfname,
            quote = F,sep="\t",row.names = F)
system(paste("~/google-cloud-sdk/bin/gsutil cp", txtname,
             "gs://bic_data_analysis/pass1a/pheno_dmaqc/"))
system(paste("~/google-cloud-sdk/bin/gsutil cp", rdataname,
             "gs://bic_data_analysis/pass1a/pheno_dmaqc/"))
system(paste("~/google-cloud-sdk/bin/gsutil cp", dictfname,
             "gs://bic_data_analysis/pass1a/pheno_dmaqc/"))
system(paste("~/google-cloud-sdk/bin/gsutil cp", 
             "~/Desktop/repos/motrpac/animal_data/README.txt",
             "gs://bic_data_analysis/pass1a/pheno_dmaqc/"))

system(paste("rm", txtname))
system(paste("rm", rdataname))
system(paste("rm", dictfname))
```

