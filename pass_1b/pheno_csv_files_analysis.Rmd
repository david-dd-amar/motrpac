---
title: 'PASS1B DMAQC data: analysis by BIC'
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document:
    number_sections: yes
---

```{r,message=FALSE,warning=FALSE,results="hide"}
# Set the working directory to the folder with the data
source("~/Desktop/repos/motrpac-bic-norm-qc/tools/gcp_functions.R")

# Load data from the from the buckets
local_path = "~/Desktop/MoTrPAC/data/pass_1b/dmaqc_pheno/"
system(paste("mkdir",local_path))
bucket = "gs://motrpac-portal-transfer-dmaqc/DMAQC_TRANSFER_PASS_1B.6M_1.00/"
dmaqc_data_dir = paste(local_path,"3-Data_Sets/",sep="")
# dictionary path
dmaqc_dict_dir = paste(local_path,"1-Data_Dictionary/",sep="")
download_bucket_files_to_local_dir(bucket = paste(bucket,"3-Data_Sets/",sep=""),
                                   local_path = dmaqc_data_dir)
download_bucket_files_to_local_dir(bucket = paste(bucket,"1-Data_Dictionary/",sep=""),
                                   local_path = dmaqc_dict_dir)

all_csvs = list.files(dmaqc_data_dir,full.names = T) # get all files in dir
all_csvs = all_csvs[grepl(".csv$",all_csvs)] # make sure we take csv only
# read all files
csv_data = list()
for(fname in all_csvs){
  fname_wo_path = strsplit(fname,split='/')[[1]]
  fname_wo_path = fname_wo_path[length(fname_wo_path)]
  csv_data[[fname_wo_path]] = read.csv(fname,stringsAsFactors = F)
}# sapply(csv_data,dim) # check the dimensions of the different datasets

all_dict_csvs = list.files(dmaqc_dict_dir,full.names = T) # get all files in dir
all_dict_csvs = all_dict_csvs[grepl(".csv$",all_dict_csvs)] # make sure we take csv only
# read all files
dict_data = list()
for(fname in all_dict_csvs){
  fname_wo_path = strsplit(fname,split='/')[[1]]
  fname_wo_path = fname_wo_path[length(fname_wo_path)]
  dict_data[[fname_wo_path]] = read.csv(fname,stringsAsFactors = F)
} 

# Shorten the csv names
names(csv_data) = 
  sapply(names(csv_data),function(x)strsplit(x,split="PASS_Animal.")[[1]][2])
names(dict_data) = 
  sapply(names(dict_data),function(x)strsplit(x,split="PASS_Animal.")[[1]][2])
names(csv_data) = gsub(".csv","",names(csv_data))
names(dict_data) = gsub(".csv","",names(dict_data))

# Reorder the datasets in the following order: 
# pid-based, labelid-based, bid-based, vial-based, BIC
neword = c(
  "Key","Registration","Familiarization",
  "NMR.Testing", "Training", "Terminal.Weight", "VO2.Max.Test",
  "Specimen.Collection","Specimen.Processing","Calculated.Variables",
  "BICLabelData"
)
csv_data = csv_data[neword]
dict_data = dict_data[neword]
sapply(csv_data,function(x)colnames(x)[1:2])
sapply(csv_data, nrow)
sapply(csv_data,function(x)"pid" %in% names(x))
sapply(csv_data,function(x)"labelid" %in% names(x)) # use labelid in data
sapply(csv_data,function(x)"labelID" %in% names(x)) # use labelID in the BIC's sheet?
sapply(csv_data,function(x)"bid" %in% names(x))
sapply(csv_data,function(x)"vialLabel" %in% names(x))

library(stringr)
FIELD_NAME_PREF = "PASS1BF" # Prefix for the field names we generate
FIELD_NUMERIC_WIDTH = 5 # numeric part size after padding
FIELD_COUNT = 1
generate_new_field_id = function(){
  newf = paste0(FIELD_NAME_PREF,str_pad(FIELD_COUNT,FIELD_NUMERIC_WIDTH,"left","0"))
  FIELD_COUNT = FIELD_COUNT + 1
  assign("FIELD_COUNT", FIELD_COUNT, envir = .GlobalEnv)
  return(newf)
}
ID_COLS = c(
 "pid","bid","labelid","viallabel"
)
names(ID_COLS) = sapply(ID_COLS,function(x)generate_new_field_id())
```

NOTE: in the code above we ignored the site-level tables. These include additional information such as temperature and humidity.

We now check field overlaps between the dictionaries. 
Let $S$ be the set of datasets. For a dataset $i=1,...,|S|$ define a set of fields $R_i$ as the set of field names that already appeared in datasets $1...i-1$. Use the fieldGUID to check for field overlaps. $R_i$ will be used later when the tables are merged. 

```{r}
R = list()
for(n1 in names(dict_data)){
  R_n1 = c()
  for(n2 in names(dict_data)){
    if(n1==n2){break}
    inter = intersect(dict_data[[n1]][,"Field.Name"],
                      dict_data[[n2]][,"Field.Name"])
    if(length(inter)==0){next}
    # print(paste(n1,n2,sep=" ### "))
    # print(inter)
    if(any(grepl("fieldGUID",names(dict_data[[n1]]))) &&
       any(grepl("fieldGUID",names(dict_data[[n2]])))){
          inter = intersect(dict_data[[n1]][,13],
                      dict_data[[n2]][,13])
          if(length(inter)==0){next}
          # print(paste(n1,n2,sep=" ### "))
          # print(inter)
          R_n1 = union(R_n1,
                 unique(dict_data[[n1]][dict_data[[n1]][,13] %in% inter,"Field.Name"]))
       }
  }
  R[[n1]] = R_n1
}
R
```

We go over the data dictionary and mark the fields that we need to keep. To this end we decided to keep fields with codes 1 and 2 in the code for release, which appears in column 1 in the dictionary. We also decided to exclude fields associated with the CRF version used.

```{r}
new_dicts = list()
for(dictname in names(dict_data)){
  dict = dict_data[[dictname]]
  id_inds = dict$Field.Name %in% ID_COLS
  dict[["Alt.Field.Name"]] = paste(dictname,dict$Field.Name,sep=".")
  dict[["Alt.Field.Name"]][id_inds] = dict[["Field.Name"]][id_inds]
  to_keep = dict[[1]] < 3
  fnames = dict[["Field.Name"]]
  to_keep = to_keep & !grepl("version",fnames)
  dict_columns_to_keep = 
    !grepl("^External.Data.Release",names(dict),perl=T) &
    !grepl("^versionNbr",names(dict),perl=T) &
    !grepl("^Data.Set.Variable.Sequence",names(dict),perl=T) &
    !grepl("^fieldGUID",names(dict),perl=T)
  new_dicts[[dictname]] = unique(dict[to_keep,dict_columns_to_keep])
  if(any(table(new_dicts[[dictname]][,"Field.Name"])>1)){
    print(paste("In",dictname,"field names are not unique after filter"))
    print(new_dicts[[dictname]][,"Field.Name"])
  }
  if(!all(new_dicts[[dictname]][,"Field.Name"] %in% colnames(csv_data[[dictname]]))){
    print(paste("Error, not all remaining fields appear in the dataset:",dictname))
  }
}
dict_data = new_dicts
for(dataset_name in names(csv_data)){
  d = csv_data[[dataset_name]]
  d = d[,dict_data[[dataset_name]][,"Field.Name"]]
  csv_data[[dataset_name]] = d
}
sapply(csv_data, nrow)
sapply(csv_data,function(x)"pid" %in% names(x))
sapply(csv_data,function(x)"labelid" %in% names(x)) # use labelid in data
sapply(csv_data,function(x)"labelID" %in% names(x)) # use labelID in the BIC's sheet?
sapply(csv_data,function(x)"bid" %in% names(x))
sapply(csv_data,function(x)"vialLabel" %in% names(x))

# For some reason some dictionaries start with FormName and some call it FormName
sapply(dict_data,names)
for(n in names(dict_data)){
  names(dict_data[[n]])[1] = "FormName"
}
# Change the BIC - info - lowercase the field names
# for consistency with the other datasets
dict_data$BICLabelData$Field.Name = tolower(dict_data$BICLabelData$Field.Name)
colnames(csv_data$BICLabelData) = tolower(colnames(csv_data$BICLabelData))
# check for overlap between the R sets and the current names
for(currname in names(R)){
  print(intersect(R[[currname]],colnames(csv_data[[currname]])))
}
```

# Format the metadata table according to vial ids

We now merge the tables using the common column pid for animal data and labelid for specimen data.
```{r,out.height='50%',out.width='50%',eval=T}
# Define the columns to generate in the final dictionary
DICT_COLS = names(dict_data$Registration)
# helper function for adding fields to the dictionary
get_dict_subset_by_field_names<-function(newfields,d2,cols){
  m = d2[d2[,"Field.Name"] %in% newfields,]
  for(col in setdiff(cols,names(d2))){
    m[[col]] = NA
  }
  rownames(m) = m[,"Field.Name"]
  m = m[newfields,]
  rownames(m) = NULL
  return(m)
}

merged_dmaqc_data = c()
merged_dmaqc_dict = c()
# Start with merging by pid
for(currname in names(dict_data)[1:9]){
  curr_data = csv_data[[currname]]
  if(currname %in% names(R)){
    curr_data_subset = curr_data[,
         setdiff(names(curr_data),R[[currname]])]
  }
  else{
    curr_data_subset = curr_data
  }
  
  # Remove id columns that were previously added, this way
  # we keep ones that were already seen
  merged_data_covered_id_cols = intersect(ID_COLS,colnames(merged_dmaqc_data))
  covered_id_col = names(curr_data_subset) %in% merged_data_covered_id_cols
  curr_data_subset = curr_data_subset[,!covered_id_col]
  # Keep the original names and add pid if necessary
  original_names = names(curr_data_subset)
  is_pid = colnames(curr_data_subset) == "pid"
  is_id_col = colnames(curr_data_subset) %in% ID_COLS
  # Make the current dataset field names fit our Alt.Field.Name
  colnames(curr_data_subset)[!is_id_col] =
    paste(currname,colnames(curr_data_subset)[!is_id_col],sep=".")
  # Add the pid column back for the merge
  curr_data_subset[["pid"]] = curr_data[["pid"]]

  if(length(merged_dmaqc_data)==0){
    merged_dmaqc_data = curr_data_subset
  }
  else{
    merged_dmaqc_data = 
      merge(merged_dmaqc_data,curr_data_subset,by="pid")
  }
  
  merged_dmaqc_dict = rbind(merged_dmaqc_dict,
        get_dict_subset_by_field_names(original_names,dict_data[[currname]],DICT_COLS)
  )
}

# Add the calculated variables using the label id
currname = "Calculated.Variables"
curr_data = csv_data[[currname]]
curr_data_subset = curr_data
# Remove id columns that were previously added
merged_data_covered_id_cols = intersect(ID_COLS,colnames(merged_dmaqc_data))
covered_id_col = names(curr_data_subset) %in% merged_data_covered_id_cols
curr_data_subset = curr_data_subset[,!covered_id_col]
# Keep the original names and add pid if necessart
original_names = names(curr_data_subset)
is_id_col = colnames(curr_data_subset) %in% ID_COLS
colnames(curr_data_subset)[!is_id_col] =
    paste(currname,colnames(curr_data_subset)[!is_id_col],sep=".")
curr_data_subset[["labelid"]] = curr_data[["labelid"]]
merged_dmaqc_data = merge(merged_dmaqc_data,curr_data_subset,by="labelid")
merged_dmaqc_dict = 
  rbind(merged_dmaqc_dict,
       get_dict_subset_by_field_names(original_names,dict_data[[currname]],DICT_COLS)
  )

print("Merged dataset dim:")
print(dim(merged_dmaqc_data))
print("Doctionary dim:")
print(dim(merged_dmaqc_dict))
# check that the field names match
print("Do column names in the data match the dictionary?")
names1 = names(merged_dmaqc_data)
names2 = merged_dmaqc_dict$Alt.Field.Name
print(all(sort(names1)==sort(names2)))
print("Are the column names unique?")
print(all(table(names1)==1))
# Reorder the dictionary to fit the data
rownames(merged_dmaqc_dict) = names2
merged_dmaqc_dict = merged_dmaqc_dict[names1,]
rownames(merged_dmaqc_dict) = NULL
```

Use DMAQC's mapping of label ids to vial ids and use it to generate a single metadata table that we can share with other sites.

```{r}
# Now map DMAQC's label ids to vialids
mapping_info = csv_data$BICLabelData
colnames(mapping_info) = tolower(colnames(mapping_info))
# Not all samples in the specimen data are necessarily covered in the mapping
# file. The mapping file contains info only about samples that were shipped
# to CAS. As can be seen here:
table(is.element(merged_dmaqc_data$labelid,set=mapping_info$labelid))
# We therefore need to extract the intersection:
shared_labelids = intersect(merged_dmaqc_data$labelid,mapping_info$labelid)
merged_dmaqc_data = merged_dmaqc_data[
  is.element(merged_dmaqc_data$labelid,set = shared_labelids),]
mapping_info = mapping_info[
  is.element(mapping_info$labelid,set = shared_labelids),c("labelid","viallabel")]
print("Merged animal and biospecimen data tables, new dim is:")
print(dim(merged_dmaqc_data))
# We also have a many to one mapping from vial ids to labels, we 
# merge the tables to avoid information loss
merged_dmaqc_data = merge(merged_dmaqc_data,mapping_info,by="labelid")
print("Merged animal and biospecimen data tables, after adding vialids, new dim is:")
print(dim(merged_dmaqc_data))
# Add the labelid to the dictionary
names(merged_dmaqc_dict)
biclabel_dict = dict_data$BICLabelData
viallabel_row = biclabel_dict[
  grepl("viallabel",biclabel_dict$Field.Name,ignore.case = T),]
v = c("","viallabel",
      paste("This is the Primary sample id.",as.character(viallabel_row[3])),
      viallabel_row[4:6],
      "viallabel",
      NA,NA,NA
)
names(v) = names(merged_dmaqc_dict)
merged_dmaqc_dict = rbind(merged_dmaqc_dict,v)

print("Merged dataset dim:")
print(dim(merged_dmaqc_data))
print("Doctionary dim:")
print(dim(merged_dmaqc_dict))
# check that the field names match
print("Do column names in the data match the dictionary?")
names1 = names(merged_dmaqc_data)
names2 = merged_dmaqc_dict$Alt.Field.Name
print(all(sort(names1)==sort(names2)))
print("Are the column names unique?")
print(all(table(names1)==1))
# Reorder the dictionary to fit the data
rownames(merged_dmaqc_dict) = names2
merged_dmaqc_dict = merged_dmaqc_dict[names1,]
rownames(merged_dmaqc_dict) = NULL

# Finally, make the ID columns the first columns that appear in the data
curr_id_col_inds = which(names2 %in% ID_COLS)
curr_other_col_inds = which(!(names2 %in% ID_COLS))
neworder = c(curr_id_col_inds,curr_other_col_inds)
merged_dmaqc_data = merged_dmaqc_data[,neworder]
merged_dmaqc_dict = merged_dmaqc_dict[neworder,]

names2 = merged_dmaqc_dict$Alt.Field.Name
curr_id_col_inds = which(names2 == "viallabel")
curr_other_col_inds = which(names2 != "viallabel")
neworder = c(curr_id_col_inds,curr_other_col_inds)
merged_dmaqc_data = merged_dmaqc_data[,neworder]
merged_dmaqc_dict = merged_dmaqc_dict[neworder,]

print("Merged dataset dim:")
print(dim(merged_dmaqc_data))
print("Doctionary dim:")
print(dim(merged_dmaqc_dict))
# check that the field names match
print("Do column names in the data the same as the dictionary fields?")
names1 = names(merged_dmaqc_data)
names2 = merged_dmaqc_dict$Alt.Field.Name
print(all(names1==names2))
```


# Sanity checks and QA

We now reread the raw data and compare it to our merged table using selected columns.

```{r}
all_csvs = list.files(dmaqc_data_dir,full.names = T) # get all files in dir
all_csvs = all_csvs[grepl(".csv$",all_csvs)] # make sure we take csv only
# read all files
csv_data = list()
for(fname in all_csvs){
  fname_wo_path = strsplit(fname,split='/')[[1]]
  fname_wo_path = fname_wo_path[length(fname_wo_path)]
  csv_data[[fname_wo_path]] = read.csv(fname,stringsAsFactors = F)
}# sapply(csv_data,dim) # check the dimensions of the different datasets
names(csv_data) = 
  sapply(names(csv_data),function(x)strsplit(x,split="PASS_Animal.")[[1]][2])
names(csv_data) = gsub(".csv","",names(csv_data))

check_two_feature_tables<-function(x1,x2){
  x1 = unique(x1)
  x2 = unique(x2)
  tests = all(x2[,1] %in% x1[,1])
  x1 = x1[x1[,1] %in% x2[,1],]
  x1 = x1[order(x1[,1],x1[,2]),]
  x2 = x2[order(x2[,1],x2[,2]),]
  tests = c(tests,all(dim(x1)==dim(x2)))
  if(!tests[2]){return(FALSE)}
  tests = c(tests,all(is.na(x1)==is.na(x2),na.rm = T))
  tests = c(tests,all(x1==x2,na.rm = T))
  return(all(tests))
}

# check the sex and weight
sex_info_1 = unique(csv_data$Registration[,c("pid","sex")])
sex_info_2 = unique(merged_dmaqc_data[,c("pid","Registration.sex")])
check_two_feature_tables(sex_info_1,sex_info_2)
w_info_1 = unique(csv_data$Registration[,c("pid","weight")])
w_info_2 = unique(merged_dmaqc_data[,c("pid","Registration.weight")])
check_two_feature_tables(w_info_1,w_info_2)

# check the randomization group
g_info_1 = unique(csv_data$Key[,c("pid","ANIRandGroup")])
g_info_2 = unique(merged_dmaqc_data[,c("pid","Key.ANIRandGroup")])
check_two_feature_tables(g_info_1,g_info_2)

# Check familiarization days_treadmillbegin and fat
dt_info_1 = unique(csv_data$Familiarization[,c("pid","days_treadmillbegin")])
dt_info_2 = unique(merged_dmaqc_data[,c("pid","Familiarization.days_treadmillbegin")])
check_two_feature_tables(dt_info_1,dt_info_2)
f_info_1 = unique(csv_data$Familiarization[,c("pid","fat")])
f_info_2 = unique(merged_dmaqc_data[,c("pid","Familiarization.fat")])
check_two_feature_tables(f_info_1,f_info_2)

# Check tissues
tissue_info_1 = unique(csv_data$Specimen.Processing[,c("labelid","sampletypedescription")])
tissue_info_2 = unique(merged_dmaqc_data[,
                  c("labelid","Specimen.Processing.sampletypedescription")])
check_two_feature_tables(tissue_info_1,tissue_info_2)

# Check VO2
vo2_info_1 = unique(csv_data$VO2.Max.Test[,c("pid","vo2_max")])
vo2_info_2 = unique(merged_dmaqc_data[,c("pid","VO2.Max.Test.vo2_max")])
check_two_feature_tables(vo2_info_1,vo2_info_2)

```

# Save the merged datasets in the cloud

```{r,eval=T}
# change the names to fit pass1a
names1 = colnames(merged_dmaqc_data)
names2 = merged_dmaqc_dict$Alt.Field.Name
all(names1==names2)
animal_fields = grepl("^(Key|Registration|Famili|NMR|Training|VO2)",names1,perl = T)
names1[animal_fields] = paste0("Animal.",names1[animal_fields])
names1 = tolower(names1)
merged_dmaqc_dict$Alt.Field.Name = names1
colnames(merged_dmaqc_data) = names1

currdate = Sys.Date()
txtname = paste("merged_dmaqc_data",currdate,".txt",sep="")
rdataname = paste("merged_dmaqc_data",currdate,".RData",sep="")
dictfname = paste("merged_dmaqc_dict",currdate,".txt",sep="")
write.table(merged_dmaqc_data,file=txtname,quote = F,sep="\t",row.names = F)
save(merged_dmaqc_data,file=rdataname)
write.table(merged_dmaqc_dict,file=dictfname,quote = F,sep="\t",row.names = F)
system(paste("~/google-cloud-sdk/bin/gsutil cp", txtname,
             "gs://bic_data_analysis/pass1b/pheno_dmaqc/"))
system(paste("~/google-cloud-sdk/bin/gsutil cp", rdataname,
             "gs://bic_data_analysis/pass1b/pheno_dmaqc/"))
system(paste("~/google-cloud-sdk/bin/gsutil cp", dictfname,
             "gs://bic_data_analysis/pass1b/pheno_dmaqc/"))

# Compare to the prev version
x1 = merged_dmaqc_data
x2 = load_from_bucket("merged_dmaqc_data2020-04-29.RData","gs://bic_data_analysis/pass1b/pheno_dmaqc/")
x2 = x2$merged_dmaqc_data
table(x2==x1)
```


# Remaining issues and questions

## For DMAQC

1. Should we keep d_visit and similar fields? for example for pid's with more than a single vo2 max score
2. FormName vs. Sample.Type
3. Some tables do not have the fieldguid
4. Okay to ignore the site-level csvs?
5. Can we assume that the fields are exactly the same for each project, e.g., 1B 6M vs. 1B 18M.

## For BIC

1. Should we create shoertened field names. We can do something like "PASS1BF00001" etc.
2. Can we compare to the CAS metadata files? 

